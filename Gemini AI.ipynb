{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db69f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Using cached google_api_core-2.24.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Using cached google_api_python_client-2.162.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting pydantic (from google-generativeai)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting tqdm (from google-generativeai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions (from google-generativeai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Using cached proto_plus-1.26.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Using cached googleapis_common_protos-1.68.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting requests<3.0.0.dev0,>=2.18.0 (from google-api-core->google-generativeai)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google-generativeai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic->google-generativeai)\n",
      "  Using cached pydantic_core-2.27.2-cp313-cp313-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\aresa\\onedrive\\desktop\\gemini ai\\env\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Using cached grpcio-1.70.0-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Using cached grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai)\n",
      "  Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "Using cached google_api_core-2.24.1-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached google_api_python_client-2.162.0-py2.py3-none-any.whl (13.1 MB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.68.0-py2.py3-none-any.whl (164 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Using cached proto_plus-1.26.0-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Using cached grpcio-1.70.0-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "Using cached grpcio_status-1.70.0-py3-none-any.whl (14 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, uritemplate, typing-extensions, tqdm, pyparsing, pyasn1, protobuf, idna, grpcio, charset-normalizer, certifi, cachetools, annotated-types, rsa, requests, pydantic-core, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, pydantic, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 google-ai-generativelanguage-0.6.15 google-api-core-2.24.1 google-api-python-client-2.162.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.4 googleapis-common-protos-1.68.0 grpcio-1.70.0 grpcio-status-1.70.0 httplib2-0.22.0 idna-3.10 proto-plus-1.26.0 protobuf-5.29.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.10.6 pydantic-core-2.27.2 pyparsing-3.2.1 requests-2.32.3 rsa-4.9 tqdm-4.67.1 typing-extensions-4.12.2 uritemplate-4.1.1 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2acce02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c06653ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\aresa\\OneDrive\\Desktop\\Gemini AI\\Gemini AI API KEY.txt\")\n",
    "key = f.read()\n",
    "#print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d2b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key='AIzaSyADuNh9A0T0k7uPSXrWtObtTKDpoOuBE0M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c71125e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object list_models at 0x000002990BB92110>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genai.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "570bc996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for model in genai.list_models():\n",
    "        print(model.name)\n",
    "except Exception as e:\n",
    "    print(f\"the error is {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82b37284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with the name of the GenAI course you completed.  There are many courses on Generative AI (GenAI), and a summary will depend heavily on the specific curriculum.  Knowing the course name will allow me to give you a much more accurate and helpful summary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash',)\n",
    "prompt = 'I completed my genai course,so give me the summary of genai'\n",
    "\n",
    "res = model.generate_content(prompt)\n",
    "print(res.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f93ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a233b292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff1bbd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models; predict and generate human-like text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = input('enter your text:')\n",
    "\n",
    "res = chat.send_message(user_input)\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fab130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for **Large Language Model**.  It's a type of artificial intelligence (AI) that can understand and generate human-like text.  These models are trained on massive datasets of text and code, allowing them to learn patterns, relationships, and nuances of language.  This enables them to perform a wide range of tasks, including:\n",
      "\n",
      "* **Text generation:**  Writing stories, articles, summaries, code, and more.\n",
      "* **Translation:**  Converting text from one language to another.\n",
      "* **Question answering:**  Providing answers to questions posed in natural language.\n",
      "* **Chatbots:**  Engaging in conversations with users.\n",
      "* **Summarization:**  Condensing large amounts of text into shorter summaries.\n",
      "* **Code generation:**  Writing code in various programming languages.\n",
      "* **Sentiment analysis:**  Determining the emotional tone of a piece of text.\n",
      "\n",
      "\n",
      "**How LLMs Work:**\n",
      "\n",
      "LLMs are typically based on neural network architectures, most commonly transformer networks.  These networks process input text sequentially, paying attention to the relationships between different words and phrases.  They learn to predict the probability of the next word in a sequence, given the preceding words.  This predictive ability is what allows them to generate coherent and contextually relevant text.\n",
      "\n",
      "**Key Characteristics:**\n",
      "\n",
      "* **Scale:**  LLMs are \"large\" because they are trained on massive datasets, often containing billions or trillions of words.  This scale is crucial for their performance.\n",
      "* **Pre-training:**  They undergo a pre-training phase where they learn general language patterns from the vast datasets.\n",
      "* **Fine-tuning:**  After pre-training, they can be fine-tuned on more specific datasets to improve their performance on particular tasks.\n",
      "* **Parameter count:**  LLMs have a huge number of parameters (weights and biases within the neural network) that determine their behavior.  More parameters generally lead to better performance, but also require more computational resources.\n",
      "\n",
      "\n",
      "**Examples of LLMs:**\n",
      "\n",
      "* **GPT-3 (and GPT-4) from OpenAI:**  One of the most well-known and powerful LLMs, known for its impressive text generation capabilities.\n",
      "* **LaMDA from Google:**  Used in Google's conversational AI applications.\n",
      "* **PaLM from Google:**  Another large and powerful model known for its reasoning abilities.\n",
      "* **LLaMA from Meta:**  A powerful open-source LLM.\n",
      "\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "While powerful, LLMs have limitations:\n",
      "\n",
      "* **Bias:**  They can reflect biases present in their training data.\n",
      "* **Hallucinations:**  They can sometimes generate incorrect or nonsensical information.  This is also known as \"fabricating facts.\"\n",
      "* **Computational cost:**  Training and running LLMs require significant computational resources.\n",
      "* **Ethical concerns:**  Their potential for misuse, such as generating misinformation or harmful content, needs careful consideration.\n",
      "\n",
      "\n",
      "In summary, LLMs are a transformative technology with the potential to revolutionize many aspects of how we interact with computers and information. However, it's crucial to be aware of their limitations and potential risks.\n",
      "\n",
      "\"GenAI\" is a shorthand term for **Generative AI**.  It's not a specific technology but rather a broad category encompassing various artificial intelligence (AI) models and techniques capable of generating new content, rather than simply analyzing or classifying existing data.  This generated content can take many forms, including:\n",
      "\n",
      "* **Text:** Stories, articles, poems, code, scripts, summaries, translations, etc.  This is often done using Large Language Models (LLMs).\n",
      "* **Images:** Realistic photos, artistic renderings, illustrations, etc., often created using models like GANs (Generative Adversarial Networks), diffusion models, or VAEs (Variational Autoencoders).\n",
      "* **Audio:** Music, sound effects, speech, etc.\n",
      "* **Video:** Short clips, animations, etc.\n",
      "* **3D Models:** Objects, characters, environments, etc.\n",
      "\n",
      "\n",
      "**Key Technologies Behind GenAI:**\n",
      "\n",
      "Several different types of AI models power Generative AI applications:\n",
      "\n",
      "* **Large Language Models (LLMs):** These are the most prominent type for text generation.  They are trained on massive datasets of text and code and can generate remarkably human-like text.  Examples include GPT-3/4, LaMDA, PaLM, and LLaMA.\n",
      "\n",
      "* **Generative Adversarial Networks (GANs):**  These models consist of two neural networks: a generator that creates data and a discriminator that tries to distinguish real data from generated data.  They are commonly used for generating images, videos, and other media.\n",
      "\n",
      "* **Diffusion Models:** These models add noise to data iteratively until it's pure noise, then learn to reverse this process to generate clean data samples.  They are particularly effective for high-quality image generation.\n",
      "\n",
      "* **Variational Autoencoders (VAEs):** These models learn a compressed representation of data and can then generate new data points based on this learned representation.\n",
      "\n",
      "\n",
      "**Examples of GenAI Applications:**\n",
      "\n",
      "* **AI art generators:**  Tools like Midjourney, DALL-E 2, and Stable Diffusion create images from text descriptions.\n",
      "* **AI music composers:**  Software that generates original musical pieces.\n",
      "* **AI code generators:**  Tools like GitHub Copilot assist programmers by suggesting code completions and generating entire functions.\n",
      "* **AI-powered writing assistants:**  These tools help with writing tasks by suggesting improvements, generating text, and more.\n",
      "* **AI chatbots:**  Many advanced chatbots utilize LLMs to generate more natural and engaging conversations.\n",
      "\n",
      "\n",
      "**Limitations of GenAI:**\n",
      "\n",
      "While powerful, GenAI technologies have limitations:\n",
      "\n",
      "* **Bias:**  Models can reflect biases present in their training data.\n",
      "* **Hallucinations:**  Models can sometimes generate inaccurate or nonsensical information.\n",
      "* **Ethical concerns:**  Potential misuse for generating misinformation, deepfakes, or other harmful content.\n",
      "* **Computational cost:**  Training and running large GenAI models can be computationally expensive.\n",
      "\n",
      "\n",
      "In short, \"GenAI\" is a rapidly evolving field with vast potential but also significant challenges related to ethics, accuracy, and responsible use.\n",
      "\n",
      "Deep learning is a subfield of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data and extract higher-level features.  It's inspired by the structure and function of the human brain, although the analogy isn't perfect.\n",
      "\n",
      "Here's a breakdown of key aspects:\n",
      "\n",
      "**1. Artificial Neural Networks (ANNs):**\n",
      "\n",
      "At the heart of deep learning are ANNs. These are computational models composed of interconnected nodes (neurons) organized in layers:\n",
      "\n",
      "* **Input layer:** Receives the initial data (e.g., pixels of an image, words in a sentence).\n",
      "* **Hidden layers:** Multiple layers between the input and output layers where complex computations and feature extraction occur.  The depth (number of hidden layers) is what distinguishes deep learning from simpler neural networks.  These layers learn increasingly abstract representations of the data.\n",
      "* **Output layer:** Produces the final result (e.g., classification label, predicted value).\n",
      "\n",
      "Each connection between neurons has a weight associated with it, representing the strength of the connection.  The network learns by adjusting these weights based on the data it's trained on.\n",
      "\n",
      "**2. Learning Process:**\n",
      "\n",
      "Deep learning models learn through a process called *training*. This involves:\n",
      "\n",
      "* **Forward propagation:**  The input data passes through the network, and the output is generated.\n",
      "* **Backpropagation:**  The difference between the predicted output and the actual target value (the loss) is calculated.  This error is then propagated backward through the network, and the weights are adjusted to reduce the error.  This process is typically done using an optimization algorithm like gradient descent.\n",
      "* **Iteration:**  Steps 1 and 2 are repeated many times over the training data until the network's performance reaches a satisfactory level.  This iterative process allows the network to gradually learn increasingly complex patterns in the data.\n",
      "\n",
      "\n",
      "**3. Types of Deep Learning Architectures:**\n",
      "\n",
      "Several architectures have been developed for different tasks:\n",
      "\n",
      "* **Convolutional Neural Networks (CNNs):**  Excellent for image and video processing, they use convolutional layers to detect patterns and features in spatial data.  They are particularly good at identifying objects and features within images.\n",
      "* **Recurrent Neural Networks (RNNs):**  Well-suited for sequential data like text and time series, they have connections that loop back on themselves, allowing them to maintain context over time.  Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are advanced types of RNNs that address the vanishing gradient problem, making them more effective for longer sequences.\n",
      "* **Generative Adversarial Networks (GANs):**  Used for generating new data samples (e.g., images, text). They consist of two networks, a generator and a discriminator, competing against each other. The generator tries to create realistic data, and the discriminator tries to distinguish between real and generated data.\n",
      "* **Autoencoders:**  Used for dimensionality reduction and feature extraction.  They learn a compressed representation of the input data and can then reconstruct the input from this representation.  Useful for denoising data and anomaly detection.\n",
      "* **Transformer Networks:**  Highly effective for natural language processing tasks, they use attention mechanisms to weigh the importance of different parts of the input sequence.  These are the basis for many large language models (LLMs).\n",
      "\n",
      "\n",
      "**4. Advantages of Deep Learning:**\n",
      "\n",
      "* **Automatic feature extraction:**  Deep learning models automatically learn relevant features from raw data, reducing the need for manual feature engineering.  This is a significant advantage over traditional machine learning methods.\n",
      "* **High accuracy:**  They often achieve state-of-the-art results on many tasks.\n",
      "* **Scalability:**  They can handle large datasets and complex problems.\n",
      "\n",
      "\n",
      "**5. Disadvantages of Deep Learning:**\n",
      "\n",
      "* **Computational cost:**  Training deep learning models can require significant computational resources (powerful hardware, long training times).\n",
      "* **Data requirements:**  They typically require large amounts of labeled data for effective training.\n",
      "* **Black box nature:**  Understanding exactly how deep learning models make their decisions can be challenging.  This lack of interpretability can be a concern in some applications.\n",
      "* **Overfitting:**  Models can sometimes overfit to the training data, performing poorly on unseen data.\n",
      "\n",
      "\n",
      "Deep learning has revolutionized many fields, including image recognition, natural language processing, speech recognition, and robotics.  Its power lies in its ability to automatically learn complex patterns from raw data, leading to breakthroughs in various applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    user_input = input('enter your text:')\n",
    "    res = chat.send_message(user_input)\n",
    "    print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ace8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Sandeep!  It's nice to meet you.\n",
      "\n",
      "Hi there! How can I help you today?\n",
      "\n",
      "Large Language Model: predicts text based on training data.\n",
      "\n",
      "Deep learning uses artificial neural networks with many layers to analyze data.\n",
      "\n",
      "Live in your dream.\n",
      "\n",
      "You told me your name is Sandeep earlier in our conversation.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43menter your text:\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     res = chat.send_message(user_input)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(res.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input('enter your text:')\n",
    "    res = chat.send_message(user_input)\n",
    "    print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd7d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35fd2d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"what is llm in 10 words\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Large language models; predict and generate human-like text.\\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7212c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:what is llm in 10 words\n",
      "model:Large language models; predict and generate human-like text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entry in chat.history:\n",
    "    print(f\"{entry.role}:{entry.parts[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d109f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6fcde8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
